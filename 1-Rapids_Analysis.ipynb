{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Analysis of Whole Slide Image Nuclei \n",
    "\n",
    "<img src=\"./images/rapids.png\" alt=\"RAPIDS\" style=\"width: 200px;\"/>\n",
    "\n",
    "In the previous notebook, using Monai Label server to generate nuclear segmentations was mentioned. This data can yield rich spatial information, along with numerous other properties, such as the shapes of all the nuclei.\n",
    "There are a range of tools at our disposal when it comes to the analysis of this data and, in this section, we are going to look at a few of the tools and techniques that you can use to find new perspectives on the data you have.\n",
    "\n",
    "We will start by using the output from running HoVerNet on the H & E stained image that comes from the [Xenium](https://www.10xgenomics.com/platforms/xenium) samples [here](https://www.10xgenomics.com/products/xenium-in-situ/preview-dataset-human-breast). You could either use the HoVerNet pipeline from MONAI to run inference on a Whole Slide, or you can use MonaiLabel, which is what was done in this case. The output from the MonaiLabel segmentation was an xml file with the outlines of each nucleus represented as a polygon, using the annotation format created by Radboud UMC for their [ASAP](https://computationalpathologygroup.github.io/ASAP/) annotation tool.\n",
    "\n",
    "There are many different formats for this type of data, but they are all doing a similar thing. One interesting feature of this type of data is that it is used extensively by the geospatial community. For example, if you want to create a map of a country then some sort of polygon, or set of polygons if it spans separate regions, might be used to describe the perimeter of the country.\n",
    "\n",
    "This type of representation is useful for mapping any large 2D spaces, including tissue slides. \n",
    "\n",
    "RAPIDS and other ML frameworks use tabular data rather than raw pixels, but can handle vector data using certain extensions. The tabular data is held in DataFrames. For CPU memory, Pandas is used, but RAPIDS provides a GPU accelerated Pandas equivalent, which is cuDF (CUDA Data Frame). Pandas has a special extension named GeoPandas, which encodes the spatial data in a specific format and provides high-performance analysis functions. Similarly, cuDF has cuSpatial, which is the GPU equivalent.\n",
    "\n",
    "\n",
    "As usual, we start by importing the libraries that we'll need. As you will notice, there are a few new names here, such as cuDF, cuGraph and cuML. These are the core of the RAPIDS tools offering GPU accelerated Dataframe functionality, GPU accelerated graph analytics and GPU accelerated Machine Learning routines\n",
    "\n",
    "You will find documentation on all of these libraries and features here https://docs.rapids.ai/api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RAPIDS to unlock clinically valuable insights from Spatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##%load_ext cudf.pandas\n",
    "from cuml import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "import cuxfilter\n",
    "from cuxfilter import DataFrame, layouts\n",
    "import pandas as pd\n",
    "import cudf\n",
    "from cudf import DataFrame\n",
    "import cugraph\n",
    "import cuml\n",
    "import numpy as np\n",
    "import cuspatial\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape, point, polygon\n",
    "import cupy\n",
    "from shapely import centroid, Polygon, Point\n",
    "import rmm\n",
    "rmm.reinitialize(managed_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start by taking a look at the .xml file that MonaiLabel emitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!head -n 50 \"./data/he-registered-to-dapi-shlee-patch-151_46_7311_5368.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format needed for GeoPandas to work is a little different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to convert the xml that we have, we need to convert each cell annotation into a shapely.shape and add it into a GeoPandas dataframe, similar to the previous notebook, which is what we will do in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "# Load the file and parse the XML\n",
    "filename = \"./data/he-registered-to-dapi-shlee-patch-151_46_7311_5368.xml\"\n",
    "tree = ET.parse(filename)\n",
    "root = tree.getroot()\n",
    "\n",
    "cells=[]\n",
    "types=[]\n",
    "cx=[]\n",
    "cy=[]\n",
    "\n",
    "for child in root:\n",
    "    if \"Description\" in child.attrib:\n",
    "\n",
    "        for annotation in child:\n",
    "            #only 1 coords per annotation\n",
    "            for coords in annotation:\n",
    "                pgon = {}\n",
    "                pgon[\"type\"]= \"Polygon\"\n",
    "                pgon[\"coordinates\"] = []\n",
    "                pts = []\n",
    "                for coord in coords:\n",
    "                     pts.append((float(coord.attrib['X']), float(coord.attrib['Y'])))\n",
    "                pgon[\"coordinates\"].append(pts)\n",
    "                shp = shape(pgon)\n",
    "                cells.append(shp)\n",
    "                types.append(annotation.attrib[\"Name\"])\n",
    "                cx.append(centroid(shp).x)\n",
    "                cy.append(centroid(shp).y)\n",
    "\n",
    "\n",
    "df_slide = gpd.GeoDataFrame({'cell_type': types, 'x': cx,'y': cy, 'geometry':cells})  \n",
    "\n",
    "df_slide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you can see that we now have a dataframe with 20,000 cell polygons. The plotting can then be done for all of these points, from which you can see the structure of the original tissue section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_slide.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this view, we can't see much detail, but by filtering the view we can effectively zoom in on an area. Let's start by finding all of the cells within a specific region. \n",
    "To do this we need to use some of the features that GeoPandas provides. Shapely has been optimized for these types of query too, so the performance is very good for modest numbers of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a square polygon region within the coordinates\n",
    "region = shape({'type': 'Polygon', 'coordinates': [[(1000.0, 1000.0), (2000.0, 1000.0), (2000.0, 2000.0), (2000.0, 1000.0), (1000.0, 1000.0)]]})\n",
    "\n",
    "# obtain True False values for each row - if it falls within the region\n",
    "df_mask = df_slide.within(region)\n",
    "\n",
    "df_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the loc method to locate all rows in which the mask is True. If we plot this out we get the requested region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply the mask to filter the rows\n",
    "df_zoom = df_slide.loc[df_mask]\n",
    "\n",
    "df_zoom.plot(column='cell_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so let's imagine that we need to map each cell to a known grid that contains information about the proteins or genes found at each location. This could help to, say, accuractely map transcripts to specific cells and allow some analysis of the networks of cells and how they interact.\n",
    "\n",
    "How might we go about doing this? Well, from an algorithic point of view there are several ways of doing this but let's start with the simplest solution and work back from there. What might a 'within polygon' function look like? It's actually not a simple algorithm but luckily shapely has a 'contains' function that we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geoms = []\n",
    "geoms.append(Point(6, 7))\n",
    "geoms.append(Point(5, 2))\n",
    "geoms.append(Point(10, 8))\n",
    "geoms.append(Point(7, 14))\n",
    "\n",
    "# define a tricky polygon!\n",
    "coords = [(2, 9), (7, 1), (12, 8), (7, 16), (6, 15),(10,9), (7,4), (5,7), (7,9), (7,11), (2,9)]\n",
    "for p in geoms:\n",
    "    print(Polygon(coords).contains(p))\n",
    "\n",
    "# add the polygon to the list\n",
    "geoms.append(Polygon(coords))    \n",
    "\n",
    "# Create a GeoPandas dataframe with its Geometry column set to contain\n",
    "# the polygons and points \n",
    "df = gpd.GeoDataFrame({'shape_type': ['xy','xy','xy','xy','poly'],'geometry':geoms})\n",
    "df.plot(column='shape_type')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this works. \n",
    "\n",
    "Can you extend this so that it works for every point in the 12 x 16 grid? ([Solution](./solutions/solution3.py))\n",
    "\n",
    "Add the points and the polygons into a single geoseries column named 'geometry' with the column named 'shape_type', with values set to either 'poly' or 'xy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO \n",
    "\n",
    "# ...\n",
    "df.plot(column='shape_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the points within the polygon, we would then do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create dataframes for the polygon and points\n",
    "df1 = df[df['shape_type']=='poly']\n",
    "df2 = df[df['shape_type']=='xy']\n",
    "\n",
    "# create a join to return the points within the polygon\n",
    "df2.sjoin(df1, how=\"inner\", predicate=\"within\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...so that seems to work okay. Let's scale this up to 1,000,000 coordinates and 20532 polygons and see how it fares..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dataframe for the \n",
    "n_points = 1000\n",
    "from timeit import default_timer as timer\n",
    "x = np.linspace(0, n_points-1, num=n_points)\n",
    "y = np.linspace(0, n_points-1, num=n_points)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "start=timer()\n",
    "geoms = [Point(p[0], p[1]) for p in np.column_stack((X.reshape(-1), Y.reshape(-1)))]\n",
    "print(timer()-start)\n",
    "\n",
    "# Create a GeoPandas dataframe with its Geometry column set to contain\n",
    "# the polygons and points \n",
    "start=timer()\n",
    "types = ['xy']*(len(geoms))\n",
    "df_grid = gpd.GeoDataFrame({'shape_type':types ,'geometry':geoms})\n",
    "print(timer()-start)\n",
    "\n",
    "start=timer()\n",
    "d_within = df_grid.sjoin(df_slide, how=\"inner\", predicate=\"within\")\n",
    "print(timer()-start)\n",
    "d_within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_within.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still pretty good - most of the time is actually being spent creating the coordinate dataframe. Next let's see how it does when we increase the number of coordinates again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dataframe for the \n",
    "n_points = 3000\n",
    "from timeit import default_timer as timer\n",
    "x = np.linspace(0, n_points-1, num=n_points)\n",
    "y = np.linspace(0, n_points-1, num=n_points)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "start=timer()\n",
    "geoms = [Point(p[0], p[1]) for p in np.column_stack((X.reshape(-1), Y.reshape(-1)))]\n",
    "print(\"Time to generate points = {}\".format(timer()-start))\n",
    "\n",
    "# Create a GeoPandas dataframe with its Geometry column set to contain\n",
    "# the polygons and points \n",
    "start=timer()\n",
    "types = ['xy']*(len(geoms))\n",
    "df_grid = gpd.GeoDataFrame({'shape_type':types ,'geometry':geoms})\n",
    "print(\"Time to create polygons = {}\".format(timer()-start))\n",
    "\n",
    "start=timer()\n",
    "d_within = df_grid.sjoin(df_slide, how=\"inner\", predicate=\"within\")\n",
    "print(\"Time to find intersections = {}\".format(timer()-start))\n",
    "d_within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_within.plot(marker=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the performance is okay but with more data points everything is starting to slow down in the expected exponential fashion.\n",
    "So, what geopandas is to pandas, cuspatial is to cudf. In other words it's a GPU accelerated version of geopandas. It uses the same underlying geoseries datastructure but is ideal for very large spatial problems that would otherwise hit a performance ceiling on CPU. Although the API is not as comprehensive as the geopandas equivalent, it still has some useful features for this sort of analysis.\n",
    "\n",
    "You can create a cuspatial geoseries by pulling out the 'geometry' data from the dataframe. So we can create one for the grid and one for the polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The creation of the cuspatial datastructure actually takes quite if they are created using Shapely, but if we create a cuDF dataframe of the x/y coordinates first, this speeds things up hugely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x = [float(x) for x in range(3000) for y in range(3000)]\n",
    "y = [float(y) for x in range(3000) for y in range(3000)]\n",
    "xy_df = cudf.DataFrame({\"x\": x, \"y\": y}).interleave_columns()  # assuming you have a dataframe with X and Y coords\n",
    "cu_points = cuspatial.GeoSeries.from_points_xy(xy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are faster ways of creting the polygons too (which you can experiment with if you like), this only takes a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cu_boxes = cuspatial.GeoSeries(df_slide['geometry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can use a neat feature, similar to the spatial index in geopandas, which is to create quadtree index for the points. This makes searching through them much faster. It divides each level into a quadrant and, if there points within any of the quarters it will subdivide those recursively until the tree index is built. There are various parameters that control the speed and memory size of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns in the quadtree are:\n",
    "* key - the index to the node\n",
    "* level - how many levels down the tree this node is\n",
    "* is_internal_node - indicates whether this is a leaf node or not\n",
    "* length - the number of elements within this node\n",
    "* offset - the index of the first point in the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a quadtree with scale of 1, a depth of 12 and a maximum number of points per node of 125\n",
    "pi, qt = cuspatial.quadtree_on_points(cu_points, 0, 3000, 0, 3000, 1, 12, 125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is an index of points and the tree itself.\n",
    "To further limit the search we can generate a set of bounding boxes for each polygon (cell) and then look for intersections between points and bounding boxes. This reduces the search space by a significant factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get bounding boxes for all polygons\n",
    "poly_bboxes = cuspatial.polygon_bounding_boxes(cu_boxes)\n",
    "# Next we generate a list of these intersections using \n",
    "# the join_quadtree_and_bounding_boxes method\n",
    "intersections = cuspatial.join_quadtree_and_bounding_boxes(qt, poly_bboxes, 0, 3000, 0, 3000, 1, 12)\n",
    "# FFind all of the points in each polygon limiting \n",
    "# the search to the intersections using the quadtree points\n",
    "polygons_and_points  = cuspatial.quadtree_point_in_polygon(intersections, qt, pi, cu_points, cu_boxes)\n",
    "polygons_and_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can then plot this out to by getting the data back into Geopandas (i.e. from GPU to CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cu_points[pi[polygons_and_points['point_index']]].to_geopandas().plot(marker=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, although the plot is not very clear, it is showing us only those points that fall within a cell.\n",
    "\n",
    "RAPIDS has a lot of other features that we can use to examine the data we have so let's take a look at a few of these.\n",
    "Next, we can create a list of centroids for each nucleus, which can allow us to do some different kinds of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create a cuda dataframe (GPU) from the pandas dataframe (CPU)\n",
    "df1 = df_slide[['cell_type', 'x', 'y']]\n",
    "\n",
    "cdf = cudf.DataFrame(df1)\n",
    "cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also display some summary statistics for the dataframe columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get some stats on the data\n",
    "cdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a view of the data we can use MatplotLib to show us a scatter-plot using the x and y coordinates of the nuclei, using the type to set the colour of each point in the output. As you can see, this provides a clear indication of the distribution of the various types of nuclei across the tissue.\n",
    "First, we need to convert the cell_type into a numeric value, for which we can use the LabelEncoder feature from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "LE = LabelEncoder()\n",
    "df1['cell_type_num'] = LE.fit_transform(df1['cell_type'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a matplotlib colormap \n",
    "cmap = ListedColormap([\"blue\", \"gold\", \"lawngreen\", \"red\"])\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize = (8, 6))\n",
    "plt.scatter(df1['x'], df1['y'], s = 2, c = df1['cell_type_num'], cmap = cmap)\n",
    "\n",
    "plt.title('Nuclei coloured by type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is certainly an interesting visualisation in its own right, but it doesn't tell us much about the relationships between the cells at a more granular level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what we are going to do now is to create a graph of all the nuclei that we detected and classified. \n",
    "In order to construct the graph we are going to use the nearest neighbout algorithm to find the 5 nearest neighbours to each nucleus.\n",
    "\n",
    "To do this we will need to import some classes from cuNN (A CUDA library for nearest neighbour computation) that is part of the RAPIDS family. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cuml.neighbors import NearestNeighbors\n",
    "\n",
    "cdf = cudf.from_pandas(df1[['x', 'y','cell_type_num']])\n",
    "knn_cuml = NearestNeighbors()\n",
    "knn_cuml.fit(cdf)\n",
    "\n",
    "%time D_cuml, I_cuml = knn_cuml.kneighbors(cdf, 5)\n",
    "I_cuml, D_cuml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what this has produced is a list of the 5 nearest neighbours for each nucleus - using the row and col values, which relate to the position of the centroid of each nucleus in pixel space. The KNN (k-nearest neighbours) algorithm is using a Euclidean distance calculation (other algorithms can be used) which tells us how close each node is to every other node. Because we chose 5 as the number of nearest neighbours, we have a row value which represents the index of each node and then five columns containing the indexes of the 5 nearest nodes, with the nearest in column 0 and the furthest in column 4. You will also notice that the index in the nearest column, 0, always matches the row index. That's because the algorithm does not exclude each node from being its own nearest neighbour. We can ignore that column.\n",
    "\n",
    "We are looking at the indexes in the I_cuml dataframe. To look at the physical distances, in pixels, you should look at the D_cuml dataframe.\n",
    "\n",
    "If you want to compare this with the sklearn CPU implementation, be aware that whilst, for this many nodes, sklearn will work fine, when you start to deal with hundreds of thousands of nodes it can take 10s of minutes to run! RAPIDS is certainly your friend in such cases. \n",
    "\n",
    "In order to convert the output of the KNN operation into a graph, we need to prepare the data. The data needs to be presented to the RAPIDS graph library, cugraph, as a set of edges with the source and destination node and an optional weight parameter.\n",
    "\n",
    "Firstly we combine the nearest neighbour indexes and distances into one dataframe and give them unique column names. We do this so that we can use the distance to set the weight of the connection between the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# give the columns names because they have to be unique in the merged dataframe\n",
    "# Indexes of neighbours\n",
    "I_cuml.columns=['ix1','n1','n2','n3','n4'] \n",
    "# Distance to neighbours\n",
    "D_cuml.columns=['ix2','d1','d2','d3','d4'] \n",
    "# Concatenate the columsn into a single dataframe\n",
    "all_cols = cudf.concat([I_cuml, D_cuml],axis=1)\n",
    "\n",
    "# remove the index and distance from the self-referenced nearest neighbour\n",
    "all_cols = all_cols[['n1','n2','n3','n4','d1','d2','d3','d4']]\n",
    "\n",
    "all_cols "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the next step is to manipulate this data so that it is in the desired format. There should be 3 columns, named 'source', 'target' and 'distance'.\n",
    "\n",
    "To do this, you will need to extract 4 sets of columns - one for each neighbour - and then concatenate the rows into a new dataframe.\n",
    "\n",
    "Remember that each row index represents a node, the n1-n4 columns contain the row index of a destination node and the d1-d4 columns contain the distance between these nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reformat the data to match the way edges are defined in cuGraph\n",
    "all_cols['index1'] = all_cols.index\n",
    "\n",
    "c1 = all_cols[['index1','n1','d1']]\n",
    "c1.columns=['source','target','distance']\n",
    "c2 = all_cols[['index1','n2','d2']]\n",
    "c2.columns=['source','target','distance']\n",
    "c3 = all_cols[['index1','n3','d3']]\n",
    "c3.columns=['source','target','distance']\n",
    "c4 = all_cols[['index1','n4','d4']]\n",
    "c4.columns=['source','target','distance']\n",
    "                 \n",
    "edges = [c1,c2,c3,c4]\n",
    "\n",
    "edge_df = cudf.concat(edges)\n",
    "\n",
    "# remove the old dataframe from memory\n",
    "del(all_cols)\n",
    "\n",
    "edge_df = edge_df.reset_index()\n",
    "edge_df = edge_df[['source','target','distance']]\n",
    "edge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to set a maximum distance between connected nodes, so that we exclude any connections beyond a certain threshold. This may reveal groups of cells that are locally connected but separate from other 'cliques'. We will use a distance of 40 pixels to start off with but you can experiment with this setting.\n",
    "Note that the distance calculation actually the distance squared (to save many expensive sqrt operations) - so we need to sqaure the threshold too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "edge_df = edge_df.loc[edge_df[\"distance\"] < 40]\n",
    "edge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe is now ready to be used to generate the graph. For this we use the cugraph library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now we can actually create a graph!!\n",
    "G = cugraph.Graph()\n",
    "\n",
    "%time G.from_cudf_edgelist(edge_df,source='source', destination='target', edge_attr='distance', renumber=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the graph we can do standard graph analytical operations. Triangle Count is the number of cycles of length three. A k-core of a graph is a maximal subgraph that contains nodes of degree k or more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now we can compute some graph metrics\n",
    "count = cugraph.triangle_count(G)\n",
    "print(\"No of triangles = \" + str(count))\n",
    "\n",
    "coreno = cugraph.core_number(G)\n",
    "print(\"Core Number = \" + str(coreno))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualising the graph**\n",
    "\n",
    "One powerful feature enabled by the RAPIDS API is the visualisation of large networks. To show this in action we are going to create a chart that displays all the nuclei centroids along with the edges between their nearest neighbours.\n",
    "\n",
    "To do this we need two dataframes: One containing the nodes and their coordinates and the other with the edges and their source and target nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we only need the index of the source and target nodes\n",
    "\n",
    "# The indexes of the source and target nodes that form an edge\n",
    "edge_df = edge_df[['source','target']]\n",
    "\n",
    "# The x and y coordinates of each node (nucleus)\n",
    "nodes_ = cdf[['x','y']]\n",
    "# Vertex refers to the index of an item in the \n",
    "nodes_['vertex']=nodes_.index\n",
    "nodes_.columns=['x','y','vertex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we use cuXFilter to render the whole graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cux_df = cuxfilter.DataFrame.load_graph((nodes_, edge_df))\n",
    "\n",
    "chart0 = cuxfilter.charts.graph(edge_color_palette=['gray', 'black'],\n",
    "                                            timeout=200, \n",
    "                                            node_aggregate_fn='mean', \n",
    "                                            node_color_palette=['blue'],\n",
    "                                            edge_render_type='direct',\n",
    "                                            edge_transparency=0.5\n",
    "                                          )\n",
    "\n",
    "d = cux_df.dashboard([chart0], layout=cuxfilter.layouts.double_feature)\n",
    "\n",
    "# draw the graph\n",
    "chart0.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to use the mouse-wheel to zoom in and out of the graph plot. If you zoom in far enough you will see the individual nodes (coloured) and edges (grey lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Can you create a data frame that contains the cell type ('cell type' - as a number) , the area ('area'), and the perimeter length ('length') of all the cells? ([Solution](./solutions/solution4.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # TODO - create df1 dataframe with the specified columns\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that worked, then you should be able to plot them by cell type below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=\"area\", y=\"length\",\n",
    "    hue=\"cell_type_num\",\n",
    "    palette=sns.color_palette(\"hls\", 4),\n",
    "    data=df1,\n",
    "    legend=\"full\",\n",
    "    alpha=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is not terribly insightful in this case, but seaborn has many types of plot that you can use and there is also PCA and T-SNE provided by cuDF and sklearn that you can play around with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
