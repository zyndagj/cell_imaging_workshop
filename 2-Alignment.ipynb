{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "588511be",
   "metadata": {},
   "source": [
    "# Part 3 - Using RAPIDS and Accelerated tools to Align Spatial Transcriptomics Images\n",
    "\n",
    "When it comes to Spatial Transciptomics, there is usually a need to align channels that contain different information, such as Z-planes, DAPI and H&E stained tissue. DAPI (or, to use its less catchy name, 4â€²,6-diamidino-2-phenylindole) is a fluorescent stain channel whilst H&E (Hematoxylin and Eosin) staining picks out cell nuclei and extracellular material respectively.\n",
    "During the sample preparation process adjacent slices may be stained with these two methods and images acquired by a slide scanner. However, the resulting images will need registering to counter the effects distortions that might occur during the preparation and scanning process.\n",
    "\n",
    "This presents a few challenges:\n",
    "* The images are very large\n",
    "* The distortions and not rigid/affine\n",
    "* The images are quite dissimilar\n",
    "\n",
    "Nevertheless, it is possible to register these images with high accuracy, albeit after a lot or expertly annotated co-registration anchor points have been defined, which is extremely labour intensive, and then some intensive computation to actually perform the image transformation.\n",
    "\n",
    "In this part of the lab we are going to consider some different techniques to perform this alignment process which could be used to expedite this process.\n",
    "\n",
    "We could use generic cell segmentor, such as those in the previous notebooks, to identify potential cell outlines and then compute the centroids of each cell for each of the images. Although the two sets of centroids will not match exactly, it should be possible to find a best-fit that could generate a vector map with which to register the images.\n",
    "\n",
    "We won't have time in this lab to explore more sophisticated Deep Learning approaches but hopefully you will have been given sufficient momentum to try out your own experiments. However, it is also expected that future workshops will cover this topic, so stay tuned..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a78b8d2",
   "metadata": {},
   "source": [
    "Let's create an image which we are going to deform and a deformation vector and see how we can apply this to the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0c911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "\n",
    "# create the 2D image\n",
    "imgrid = np.zeros((256,256,3),dtype=np.uint8)\n",
    "imgrid[:,:,:] = 255\n",
    "\n",
    "# draw some lines on the image\n",
    "for x in range(1,13):\n",
    "    imgrid[20*x,:,0:2] = 0\n",
    "    imgrid[:,20*x,0:2] = 0\n",
    "\n",
    "plt.imshow(imgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a2220d",
   "metadata": {},
   "source": [
    "Imagine that we have pairs of coordinates, that represent: expected coordinates of features; and the actual location of these features. The difference between the two is a vector and we can show this vector on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f26704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a grid of coordinates, from 0-250 with 26 intervals\n",
    "x = np.linspace(0, 250, num=26)\n",
    "y = np.linspace(0, 250, num=26)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# create a vertical and horizontal value at each grid point\n",
    "vx = np.zeros(X.shape)\n",
    "vy = np.zeros(X.shape)\n",
    "\n",
    "# create a sparse set of points with x and y values\n",
    "vectors = np.array([[5.,7.],[-5.5,-3.3],[-7.5,3.]])\n",
    "coords = np.array([[20,20],[180,50],[20, 180]])\n",
    "\n",
    "# populate the vector and scale the coordinates\n",
    "for i,c in enumerate(coords):\n",
    "    vx[c[0]//10][c[1]//10]=vectors[i,0]\n",
    "    vy[c[0]//10][c[1]//10]=vectors[i,1]\n",
    "\n",
    "# creating plot\n",
    "fig, ax = plt.subplots(figsize =(5, 5))\n",
    "ax.quiver(X, Y, vx, vy,angles='xy', scale_units='xy', scale=1)\n",
    " \n",
    "ax.axis([0, 250, 0, 250])\n",
    "ax.set_aspect('equal')\n",
    " \n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d807aa34",
   "metadata": {},
   "source": [
    "This plot shows the vector we just created mapped by its coordinates. If we know that an image needs to be adjusted by this vector at these locations then it makes sense to smoothly interpolate the vectors for the points between these coordinates. There are a number of algorithms that we can use for this and quite a few tools support various implementations. SciPy has a linear interpolator that we could use here. \n",
    "\n",
    "We can treat each axis separately and follow these steps:\n",
    "* Create a grid of values that equate to the locations of the pixels in the image that we want to warp, using linspace and meshgrid\n",
    "* Create an interpolator, using the vectors we defined and the grid\n",
    "* Generate the vectors for the whole grid\n",
    "* Plot them out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab602d7",
   "metadata": {},
   "source": [
    "To make things a little more clear, we can create a function to map the vector direction to a specific color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb19a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.colors\n",
    "\n",
    "def vector_to_rgb(x,y):\n",
    "    \n",
    "    angle = np.arctan2(y,x)\n",
    "\n",
    "    # normalize angle\n",
    "    angle = angle % (2 * np.pi)\n",
    "    if angle < 0:\n",
    "        angle += 2 * np.pi\n",
    "\n",
    "    return matplotlib.colors.hsv_to_rgb((angle / 2 / np.pi, 1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1905ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import LinearNDInterpolator\n",
    "\n",
    "# interpolate the x-axis values\n",
    "interpx = LinearNDInterpolator(list(coords), vectors[:,0])\n",
    "Zx = interpx(Y, X)\n",
    "\n",
    "# interpolate the y-axis values\n",
    "interpy = LinearNDInterpolator(list(coords), vectors[:,1])\n",
    "Zy = interpy(Y, X)\n",
    "\n",
    "# creating plot\n",
    "fig, ax = plt.subplots(figsize =(5, 5))\n",
    "c1 = np.array(list(map(vector_to_rgb, Zx.flatten(),Zy.flatten())))\n",
    "ax.quiver(X, Y, Zx, Zy,angles='xy', scale_units='xy', scale=1, color=c1)\n",
    " \n",
    "ax.axis([0, 255, 0, 255])\n",
    "ax.set_aspect('equal')\n",
    " \n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dd4298",
   "metadata": {},
   "source": [
    "You will notice that the interpolation is only done for the points that lie within the region defined by the vector locations. This begs the question: What happens to the rest of the points? Well, they could be extrapolated, but this can give unexpected results since there is not enough information to work from for points that live outside the interpolation region. There are a few approaches that could be taken here:\n",
    "* Set each pixel to the vector of its nearest neighbour\n",
    "* Set each vector at the extremities of the image to 0,0 and interpolate the unspecified values\n",
    "* Use the gradient of the vector to extend the values to the borders\n",
    "* Set all undefined values to a fixed value\n",
    "\n",
    "There are pros and cons of each of these approaches and it may not matter too much in the real world since it is likely that you would have defined anchor points within all regions of the image that actually matter and so, any that fall outside this region are probably background regions of less relevance. Nevertheless, it's useful to understand the implications of this because some of the choices above could lead to unexpected results. Let's try a couple of options to illustrate this. First, we will set the points at the corners of the image to 0 and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9532f3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create some vectors at certain points\n",
    "vectors = np.array([[5.,7.],[-5.5,-3.3],[-7.5,3.],[0,0],[0,0],[0,0], [0,0]])\n",
    "coords = np.array([[20,20],[180,50],[20, 180],[0,0],[0,255],[255,0], [255,255]])\n",
    "\n",
    "interpx = LinearNDInterpolator(list(coords), vectors[:,0])\n",
    "Zxi = interpx(Y, X)\n",
    "\n",
    "interpy = LinearNDInterpolator(list(coords), vectors[:,1])\n",
    "Zyi = interpy(Y, X)\n",
    "\n",
    "# creating plot\n",
    "fig, ax = plt.subplots(figsize =(5, 5))\n",
    "c1 = np.array(list(map(vector_to_rgb, Zxi.flatten(),Zyi.flatten())))\n",
    "ax.quiver(X, Y, Zxi, Zyi,angles='xy', scale_units='xy', scale=1, color=c1)\n",
    " \n",
    "ax.axis([0, 255, 0, 255])\n",
    "ax.set_aspect('equal')\n",
    " \n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e3f42",
   "metadata": {},
   "source": [
    "So there are many different interpolation methods and each has its own characteristics. For example, the one below uses a Clough-Tocher scheme, which has somewhat different characteristics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e42df4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import CloughTocher2DInterpolator as CT\n",
    "# create some vectors at certain points\n",
    "\n",
    "interpx = CT(list(coords), vectors[:,0])\n",
    "Zxi = interpx(Y, X)\n",
    "\n",
    "interpy = CT(list(coords), vectors[:,1])\n",
    "Zyi = interpy(Y, X)\n",
    "\n",
    "# creating plot\n",
    "fig, ax = plt.subplots(figsize =(5, 5))\n",
    "c1 = np.array(list(map(vector_to_rgb, Zxi.flatten(),Zyi.flatten())))\n",
    "ax.quiver(X, Y, Zxi, Zyi,angles='xy', scale_units='xy', scale=1, color=c1)\n",
    " \n",
    "ax.axis([0, 255, 0, 255])\n",
    "ax.set_aspect('equal')\n",
    " \n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e765d2",
   "metadata": {},
   "source": [
    "Of course, detecting the global transformation needed is only part of the solution to the registration problem - the other part of it is to actually deform the source image to map the destination. To do this we need to use a warp function. Again, there are many algorithms out there that can be used, such as those found in skimage's transform class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab9ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import PiecewiseAffineTransform, warp\n",
    "    \n",
    "step_size = 20\n",
    "vectors = np.array([[3.0,1.0],[-5.,-1.3],[-3.5,8.3],[0,0],[0,0],[0,0], [0,0]])\n",
    "coords = np.array([[20,20],[180,50],[20, 180],[0,0],[0,255],[255,0], [255,255]])\n",
    "\n",
    "# Creating grid\n",
    "x = np.linspace(0, 255, num=step_size)\n",
    "y = np.linspace(0, 255, num=step_size)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "interpx = LinearNDInterpolator(list(coords), vectors[:,0])\n",
    "Zxi = interpx(Y, X)\n",
    "\n",
    "interpy = LinearNDInterpolator(list(coords), vectors[:,1])\n",
    "Zyi = interpy(Y, X)\n",
    "\n",
    "src = np.column_stack((X.reshape(-1), Y.reshape(-1)))\n",
    "\n",
    "# add the interpolated offets\n",
    "dst_rows = X + Zxi\n",
    "dst_cols = Y + Zyi\n",
    "\n",
    "dst = np.column_stack([dst_cols.reshape(-1), dst_rows.reshape(-1)])\n",
    "\n",
    "tform = PiecewiseAffineTransform()\n",
    "\n",
    "start = timer()\n",
    "tform.estimate(src, dst)\n",
    "end = timer()\n",
    "t = end-start\n",
    "print(\"cpu estimate took {}s\".format(t))\n",
    "start = timer()\n",
    "out = warp(imgrid, tform, output_shape=(255, 255))\n",
    "end = timer()\n",
    "t = end-start\n",
    "print(\"cpu warp took {}s\".format(t))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(out)\n",
    "ax.axis((0, 255, 255, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee748f9",
   "metadata": {},
   "source": [
    "Next we will create a fictitious transformation and apply it to a real image to see the problem we might be faced with. What we will do it to create a small rotation to each coordinate, but will add some random noise as well. We will then sample some points from this function, generate the transform that best fits this data and apply it to the H & E image from a pair of H & E and DAPI images that have been pre-registered. This will give us a 'known' tranformation that we can try to detect and correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb243160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate a grid\n",
    "y = np.linspace(-3, 3)\n",
    "x = np.linspace(-3, 3)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# vector direction based on position\n",
    "p = np.arctan2(X,Y)\n",
    "d = np.sqrt(X**2 + Y**2)\n",
    "# strength of vector based on distance from centre\n",
    "u = np.cos(p)*d \n",
    "v = -np.sin(p)*d \n",
    "\n",
    "fig, ax = plt.subplots(figsize =(5, 5))\n",
    "c1 = np.array(list(map(vector_to_rgb, u.flatten(),v.flatten())))\n",
    "ax.quiver(X, Y, u, v, angles='xy', scale_units='xy', scale=3, color=c1)\n",
    " \n",
    "ax.axis([-3, 3, -3, 3])\n",
    "ax.set_aspect('equal')\n",
    " \n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec5003",
   "metadata": {},
   "source": [
    "You can see that the further the coordinate is away from the center, the more it is offset. Obviously we could do this with a simple rotation, but where's the fun in that and anyway, this is just one part of the transformation.\n",
    "To add noise, we can just add a small random number to each coordinate's x and y values.\n",
    "\n",
    "In the more realistic case below, we sample 200 points and displace them using the process outlined above using a 100 x 100 grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be39dbfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "\n",
    "# level of noise\n",
    "g=0.3\n",
    "\n",
    "#number of points\n",
    "n=200\n",
    "\n",
    "# create grid\n",
    "grid_x, grid_y = np.meshgrid(np.linspace(-3, 3, 100),\n",
    "                             np.linspace(-3, 3, 100), indexing='ij')\n",
    "\n",
    "# create some points to compute\n",
    "points = np.random.rand(n,2)*6\n",
    "points = points-3.\n",
    "noise = np.random.rand(n,2)*g\n",
    "\n",
    "#for each point compute a vector\n",
    "p = np.arctan2(points[:,0],points[:,1])\n",
    "d = np.sqrt(points[:,0]**2 + points[:,1]**2)\n",
    "u = np.cos(p)*d + noise[:,0]\n",
    "v = -np.sin(p)*d + noise[:,1]\n",
    "\n",
    "# interpolate the vector for the whole grid from the points\n",
    "print(p.shape,u.shape,grid_x.shape,grid_y.shape)\n",
    "plot_x = griddata(points, u, (grid_x, grid_y), method='linear',fill_value=0.0)\n",
    "plot_y = griddata(points, v, (grid_x, grid_y), method='linear',fill_value=0.0)\n",
    "\n",
    "angles = p\n",
    "lengths = d\n",
    "max_abs = np.max(d)\n",
    "\n",
    "# color is direction, hue and value are magnitude\n",
    "c1 = np.array(list(map(vector_to_rgb, plot_x.flatten(), plot_y.flatten())))\n",
    "\n",
    "fig, ax = plt.subplots(figsize =(10, 10))\n",
    "ax.quiver(grid_x, grid_y, plot_x, plot_y,angles='xy', scale_units='xy', scale=10.0,pivot='mid',color=c1)\n",
    " \n",
    "ax.axis([-3.0, 3.0, -3.0, 3.0])\n",
    "ax.set_aspect('equal')\n",
    " \n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca92183",
   "metadata": {},
   "source": [
    "Of course, the more grid points we need to interpolate the slower the computation. To get a feel for this, we can experiment with the number of data points and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a1b2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tp = 8\n",
    "n = 80\n",
    "times = [0] * tp\n",
    "ns = [0] * tp\n",
    "\n",
    "for i in range(tp):\n",
    "    \n",
    "    grid_x, grid_y = np.meshgrid(np.linspace(-3, 3, n),\n",
    "                             np.linspace(-3, 3, n*2), indexing='ij')\n",
    "    \n",
    "    # create some points to compute\n",
    "    points = np.random.rand(n,2)*6\n",
    "    points = points-3.\n",
    "    noise = np.random.rand(n,2)*g\n",
    "\n",
    "    #for each point compute a vector\n",
    "    p = np.arctan2(points[:,0],points[:,1])\n",
    "    d = np.sqrt(points[:,0]**2 + points[:,1]**2)\n",
    "    u = np.cos(p)*d + noise[:,0]\n",
    "    v = -np.sin(p)*d + noise[:,1]\n",
    "\n",
    "    start = timer()\n",
    "    # interpolate the vector for the whole grid from the points\n",
    "    plot_x = griddata(points, u, (grid_x, grid_y), method='cubic')\n",
    "    plot_y = griddata(points, v, (grid_x, grid_y), method='cubic')\n",
    "\n",
    "    end = timer()\n",
    "    times[i] = end-start\n",
    "    ns[i]=n\n",
    "    print(n,times[i])\n",
    "    n *=2\n",
    "\n",
    "plt.plot(ns, times, '-ok')\n",
    "plt.xlabel(\"n-points\")\n",
    "plt.ylabel(\"Compute time Time (s)\");\n",
    "plt.yscale(\"linear\")\n",
    "\n",
    "plt.show    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5cdc79",
   "metadata": {},
   "source": [
    "This is actually pretty good, considering that doubling the number of coordinates is quadrupling the number of data points in the grid. It seems to increase by a factor of 4, which is expected, but we don't see any drop off in this scaling even up to 10k x 10k grid size. Nevertheless, ~50 seconds is quite a long time and precludes any sort of real-time interaction - especially if we want to work with larger images.\n",
    "Let's try some other ways of computing the same thing - i.e. a pixel-level vector that could be used to warp an image to align with another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab322a1b-93dc-463a-82a6-cc8abc381b64",
   "metadata": {},
   "source": [
    "Next we will load the H&E slide from the Xenium sample dataset. This dataset is associated with the preprint \"High resolution mapping of the breast cancer tumor microenvironment using integrated single cell, spatial and in situ analysis of FFPE tissue\", posted to bioRxiv November 03, 2022, in which a single breast cancer FFPE tissue block (Sample #1) was assayed with a trio of complementary technologies (Chromium, Visium, Xenium). In the manuscript revision (submitted June 23, 2023), a second breast cancer block (Sample #2) was analyzed with Xenium. See the preprint for full details on methods and results.\n",
    "In this instance we will use tifffile to load the image, but we could also use cucim, the RAPIDS mage loading and processing library, which can speed things up for .svs and .tiff images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88fbb17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cupy\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tifffile as tiff \n",
    "\n",
    "he_file1 = \"./data/he-registered-to-dapi-shlee.tif\"\n",
    "he_img = np.array(tiff.imread(he_file1))\n",
    "h1 = he_img.shape[0]\n",
    "w1 = he_img.shape[1]\n",
    "\n",
    "# Show the image\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(he_img)\n",
    "plt.title('he-registered-to-dapi-shlee.tif')\n",
    "print(\"Width = {}, Height = {}, Channels = {}\".format(w1, h1, he_img.shape[2]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dde181-5907-4aa5-b87e-f301c93fd60e",
   "metadata": {
    "tags": []
   },
   "source": [
    "So, once the image is loaded, we can apply some simulated deformation to it by generating a set of random points and then adding some rotation plus some noise (as before). Although the speed of the interpolation may not seem directly useful, it actally might be since, in some cases, images are aligned by defining anchor points on pairs of corresponding images and then deforming one of them to match the other. To do this smoothly, you need to interpolate each pixel's offset vector from the sparse points manually defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca6515-215b-4232-afd4-ad775042d61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import map_coordinates\n",
    "\n",
    "grid_y, grid_x = np.meshgrid(np.linspace(0, h1-1, h1),\n",
    "                             np.linspace(0, w1-1, w1), indexing='ij')\n",
    "g=30. # level of noise\n",
    "n=200 #number of points\n",
    "\n",
    "y = np.random.uniform(0, h1, size=n)\n",
    "x = np.random.uniform(0, w1, size=n)\n",
    "noise = (np.random.rand(n+4,2)-0.5)*g\n",
    "\n",
    "#add points at the extremities to avoid edge artefacts\n",
    "y = np.concatenate((y,[0],[0],[h1-1],[h1-1]))\n",
    "x = np.concatenate((x,[0],[w1-1],[0],[w1-1]))\n",
    "\n",
    "# compute an angle and distance from the centre\n",
    "p = np.arctan2(w1//2-x,h1//2-y)\n",
    "d = np.sqrt((h1//2-y)**2 + (w1//2-x)**2)\n",
    "# create a horizontal and verical vector\n",
    "u = np.cos(p)*d \n",
    "v = -np.sin(p)*d\n",
    "\n",
    "# set a maximum offset with respect to the image size\n",
    "umax = np.max(u)\n",
    "vmax = np.max(v)\n",
    "max_offset=120\n",
    "u = (u/umax) * 120 + noise[:,0]\n",
    "v = (v/vmax) * 120 + noise[:,1]\n",
    "\n",
    "# interpolate the vectors at the rest of the points\n",
    "plot_x = griddata((y,x), u, (grid_y, grid_x), method='linear',fill_value=0.0)\n",
    "plot_y = griddata((y,x), v, (grid_y, grid_x), method='linear',fill_value=0.0)\n",
    "\n",
    "gx = grid_x + plot_x\n",
    "gy = grid_y + plot_y\n",
    "coords = np.stack((gy,gx),axis=0)\n",
    "coords = np.concatenate((coords,np.zeros((1,h1,w1))),axis=0)\n",
    "coords = np.stack((coords,coords,coords), axis=3)\n",
    "# the mapping coords needs to have this set\n",
    "coords[2,:,:,0]=0.\n",
    "coords[2,:,:,1]=1.\n",
    "coords[2,:,:,2]=2.\n",
    "\n",
    "start = timer()\n",
    "host_image = map_coordinates(he_img, coords)\n",
    "print(\"warp image - \", timer()-start)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(host_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b095bd9-4357-427f-b38a-6c14730c59ca",
   "metadata": {},
   "source": [
    "Okay, so that worked, but let's see if we can speed up the warping, using a cuda version of the map_coordinates function. Notice that we need to convert all of the numpy arrays to cupy arrays, which copies them onto the gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a0a205-03e4-45db-8062-6de1fc41cc25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cupyx.scipy.ndimage import map_coordinates as cu_map_coordinates\n",
    "import cupy as cp\n",
    "\n",
    "cu_plot_x = cp.array(plot_x)\n",
    "cu_plot_y = cp.array(plot_y)\n",
    "\n",
    "gx = cp.array(grid_x) + cu_plot_x\n",
    "gy = cp.array(grid_y) + cu_plot_y\n",
    "coords = cp.stack((gy,gx),axis=0)\n",
    "coords = cp.concatenate((coords,cp.zeros((1,h1,w1))),axis=0)\n",
    "coords = cp.stack((coords,coords,coords), axis=3)\n",
    "coords[2,:,:,0]=0.\n",
    "coords[2,:,:,1]=1.\n",
    "coords[2,:,:,2]=2.\n",
    "\n",
    "start = timer()\n",
    "cu_he_img = cp.array(he_img)\n",
    "warped_image = cu_map_coordinates(cu_he_img, coords)\n",
    "host_image = warped_image.get()\n",
    "end = timer()\n",
    "print(\"warp image - \",end-start)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(host_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0bc505-6dc5-4bff-8dfe-157edb29a0f7",
   "metadata": {},
   "source": [
    "As you will have noticed, that produced a huge speed-up! From over a minute to less than a second!\n",
    "\n",
    "Let's visualize the image distortion by setting the red pixels to the amount of offset in the x direction and the blue pixels to the offset in the y direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b8b39-8e15-4ad8-865e-50391b46c440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec_img = np.zeros((h1,w1,3),dtype=np.uint8)\n",
    "mx = np.max(plot_x.flatten())\n",
    "mn = np.min(plot_x.flatten())\n",
    "\n",
    "vec_img[:,:,2] = (plot_x-mn)*255/(mx-mn)\n",
    "vec_img[:,:,1] = 0\n",
    "mx = np.max(plot_y.flatten())\n",
    "mn = np.min(plot_y.flatten())\n",
    "vec_img[:,:,0] = (plot_y-mn)*255/(mx-mn)\n",
    "\n",
    "plt.figure(figsize =(10, 10))\n",
    "\n",
    "# show plot\n",
    "plt.imshow(vec_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564663af-d041-4c31-a026-5c6c8293b1bb",
   "metadata": {},
   "source": [
    "And let's plot out the difference between the original and the warped image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb265e3-0c23-4a8f-8f84-26fb859d29cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(host_image-he_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0fc9a3-f3ac-4193-90f8-d6432578551f",
   "metadata": {},
   "source": [
    "You'll notice that the majority of the time was spent on the interpolation rather than the actual image warp. At the moment we are using a CPU-based interpolation. Although there are some GPU equivalents, there are none in cucim.skimage that can accept sparse points for 2D datasets. There are other libraries around that can do this sort of thing (e.g. OpenCV has many GPU-accelerated routines), but we'll look at some other techniques which might be useful to know. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c220ece5-06f1-4668-ae4e-0f20a8de6517",
   "metadata": {},
   "source": [
    "As an experiment we can create our own intepolation algorithm and see what sort of performance we can get using a couple of different techniques. First off, we could find the three sample points nearest each pixel on the image. RAPIDS has a some great solutions for this sort of computation and RAFT provides just such an algorithm.\n",
    "Although there are some even faster hueristics that could be used here, even the brute force algorithm should be well within our latency budget here.\n",
    "\n",
    "What we do below is to create a set of points that represent all the pixels in the image. We also send the set of deformation points over to the GPU and then let RAFT do it's thing. Given that there are >41 million points to evaluate against ~200 possible nearest neighbors, this could take some time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18df7b-b83b-4a77-81a6-e44213a30f83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import cupy as cp\n",
    "from pylibraft.neighbors.brute_force import knn\n",
    "\n",
    "# Create a point for every pixel in the image\n",
    "x1 = cp.linspace(0, w1-1, num=w1)\n",
    "y1 = cp.linspace(0, h1-1, num=h1)\n",
    "X,Y = cp.meshgrid(x1, y1)\n",
    "\n",
    "# Formulate the points as an array of x,y coordinates (e.g. shape = (height * width,2))\n",
    "points = cp.stack((Y.flatten(),X.flatten()),axis=-1)\n",
    "points = points.astype(cp.float32)\n",
    "\n",
    "# Add the randomly created sample points to a GPU array\n",
    "data = cp.stack((cp.array(y,dtype=cp.float32),cp.array(x,dtype=cp.float32)),axis=1)\n",
    "k = 3 # find nearest three sample points from each pixel location\n",
    "_, neighbors = knn(data, points, k)\n",
    "\n",
    "print(cp.asarray(neighbors).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc15303-cb4d-447a-a479-24d03c49596f",
   "metadata": {},
   "source": [
    "In fact, it actually only takes a seconds or two to find the 3 nearest neighbors for all 41,151,487 pixels in the image, which is pretty impressive!\n",
    "\n",
    "Something we can do - to sanity check the nearest neighbor computation, is to plot out the nearest neighbour indexes as RGB values (since we picked <255 of them and each point has three). If it's working as expected then we should see something resembling a voronoi diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312ef47-4971-4723-9eb5-da93314f5318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "nn_image = np.zeros((h1,w1,3),dtype=np.uint8)\n",
    "i=0\n",
    "\n",
    "ns = np.array(cp.asarray(neighbors).get())\n",
    "\n",
    "for y in range(h1):\n",
    "    for x in range(w1):\n",
    "        nn_image[y,x]=ns[i]\n",
    "        i+=1\n",
    "        \n",
    "plt.imshow(nn_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f901c4af-3a3c-429d-8ca2-dfb8c2f2db64",
   "metadata": {
    "tags": []
   },
   "source": [
    "So that took a little while to compute. One of the first tools we can try, to make things happen faster is to use [Numba](https://numba.pydata.org/). Numba supports a few different underlying strategies, including both GPU and CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c26fce-1649-44eb-86d6-aaa629007711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from numba import jit\n",
    "\n",
    "im = np.zeros((h1,w1,3),dtype=np.uint8)\n",
    "ns = np.array(cp.asarray(neighbors).get())\n",
    "\n",
    "@jit(nopython=True) # Set \"nopython\" mode for best performance, equivalent to @njit\n",
    "def fast_image(a,b): # Function is compiled to machine code when called the first time\n",
    "    \n",
    "    for y in range(h1):\n",
    "        for x in range(w1):\n",
    "            i = y * w1 + x\n",
    "            a[y,x]=b[i]\n",
    "        \n",
    "fast_image(im,ns)\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c483a056-1853-4fe1-a815-0f9f109c2376",
   "metadata": {},
   "source": [
    "That was definitely a bit quicker. In this case Numba was just using the CPU cores to speed things up, but it can also use the GPU. You can explore this in more detail [here](https://numba.pydata.org/numba-doc/latest/cuda/index.html).\n",
    "\n",
    "Now we can weight the vectors from the three nearest points using interpolation. This will provide us with the grid we need to perform the image warp.\n",
    "\n",
    "To apply the correct weight at each point we can linearly interpolate the values by thinking of the triangle formed between the three nearest neighbours as a 2D projection of a triangle lying on a plane in which the vector values form the z dimension. We can use the three points with their weight vectors as inputs to get the formula for the plane, which then allows us to plug in new x, y values and obtain the z value.\n",
    "\n",
    "<img src=\"./images/interp.png\" alt=\"Interpolation\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1887c8f-7c50-454d-bfd7-96a3b3bcaf72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_z(x,y,n,a):\n",
    "    # interpolate vector using the computed plane info\n",
    "    return  a[2]-((n[0]*(x-a[0])+n[1]*(y-a[1]))/n[2])\n",
    "\n",
    "def apply_weighted_avg(u,v,nns,x,y):\n",
    "    # add the vector as the z dimension\n",
    "    nnu = cp.column_stack((nns,u.T))\n",
    "    \n",
    "    #get the vector between two pairs of the 3 points\n",
    "    ab_u = nnu[0]-nnu[1]\n",
    "    bc_u = nnu[1]-nnu[2]\n",
    "    # ocmpute the cross product\n",
    "    n_u = cp.cross(ab_u, bc_u)\n",
    "    \n",
    "    nnv = cp.column_stack((nns,v.T))\n",
    "    ab_v = nnv[0]-nnv[1]\n",
    "    bc_v = nnv[1]-nnv[2]\n",
    "    n_v = cp.cross(ab_v, bc_v)\n",
    "   \n",
    "    ua = get_z(x,y,n_u,nnu[0])\n",
    "    va = get_z(x,y,n_v,nnv[0])\n",
    "   \n",
    "    # return the interpolated vectors (u and v) at this x and y coordinate\n",
    "    return cp.array((ua,va))\n",
    "\n",
    "# assign an array to return the result into\n",
    "grid = cp.zeros((w1,h1,2),dtype=cp.float32)\n",
    "\n",
    "# convert to GPU array\n",
    "cneighbors = cp.asarray(neighbors)\n",
    "cu = cp.array(u)\n",
    "cv = cp.array(v)\n",
    "ix=0\n",
    "\n",
    "start = timer()\n",
    "\n",
    "#iterate through all the points in the image\n",
    "for i in range(w1):\n",
    "    for j in range(h1):\n",
    "        ux=cu[cneighbors[ix]]  \n",
    "        vx=cv[cneighbors[ix]]\n",
    "        p = data[cneighbors[ix]]\n",
    "        grid[i,j] = apply_weighted_avg(ux,vx,p,i,j)\n",
    "        ix+=1\n",
    "        \n",
    "    if ix>100:\n",
    "        break\n",
    "        \n",
    "t = timer()-start\n",
    "                                                \n",
    "print(\"Computing {} points took {} seconds\".format(i,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b119c5-422e-4b85-b2e8-e5f40038f2dd",
   "metadata": {},
   "source": [
    "Oh - that could take quite a while, given that we only computed less than a 1/10000th of the full image. Luckily we can speed this up by creating a kernel (GPU function). There are several ways of doing this, from using C++ CUDA, PyCUDA or using a Python-based syntax like like Numba. Since we are already using cupy, let's see how we might do that using cupy's kernel methods.\n",
    "\n",
    "The key concept to grasp with writing kernels is that we don't loop through all the function calls in our host (CPU) code. Instead we define a function, pass (references to) the whole arrays we want to process and let the GPU figure out how to schedule the execution using the number of threads and thread blocks we specify. Within the kernel we also need to map the current thread of execution to a specific element within the arrays we have passed. Note also that we don't return anything - we pass the output in as a parameter instead. This is always the case with kernel functions.\n",
    "\n",
    "One other thing to note with this type of cupy rawkernel - we can't use array functions inside the kernel, which is why we compute the cross product 'by hand'.\n",
    "\n",
    "N.B. kernel functions never return anything. Instead we pass the array we want to work on to the function and set its elements within the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc382d5-388c-423d-a7cc-e8e18ed7905d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the Just-In-Time compiler for cupy\n",
    "from cupyx import jit\n",
    "from cupyx.profiler import benchmark\n",
    "\n",
    "# Let the compiler know that we want to create a kernel from this function\n",
    "@jit.rawkernel()\n",
    "def elementwise_uv(uv, u, v, n, d, p, size):\n",
    "    # get the current thread id \n",
    "    tid = jit.blockIdx.x * jit.blockDim.x + jit.threadIdx.x\n",
    "    # get the total number of threads available\n",
    "    ntid = jit.gridDim.x * jit.blockDim.x\n",
    "    \n",
    "    # if there are more elements to process than the total number of threads \n",
    "    # available then we neeed to loop over the array a few times\n",
    "    for i in range(tid, size, ntid):\n",
    "        if i<size:\n",
    "            # index to neighbors\n",
    "            ia = n[i,0] \n",
    "            ib = n[i,1]\n",
    "            ic = n[i,2]\n",
    "            # current point\n",
    "            yy = p[i,0]\n",
    "            xx = p[i,1]\n",
    "\n",
    "            # neighbor location (x,y)\n",
    "            a = d[ia]\n",
    "            b = d[ib]\n",
    "            c = d[ic]\n",
    "\n",
    "            # compute vector between pairs of the 3 vectors\n",
    "            ab_u = u[ia] - u[ib]\n",
    "            bc_u = u[ib] - u[ic]\n",
    "            ab_v = v[ia] - v[ib]\n",
    "            bc_v = v[ib] - v[ic]\n",
    "            # compute vector between pairs of the 3 points\n",
    "            ab_y = a[0] - b[0]\n",
    "            ab_x = a[1] - b[1]\n",
    "            bc_y = b[0] - c[0]\n",
    "            bc_x = b[1] - c[1]\n",
    "\n",
    "            # elementwise cross product computation\n",
    "            xp_ux = (ab_y * bc_u) - (ab_u * bc_y)\n",
    "            xp_uy = (ab_u * bc_x) - (ab_x * bc_u)\n",
    "            xp_uz = (ab_x * bc_y) - (ab_y * bc_x)\n",
    "            xp_vx = (ab_y * bc_v) - (ab_v * bc_y)\n",
    "            xp_vy = (ab_v * bc_x) - (ab_x * bc_v)\n",
    "            xp_vz = (ab_x * bc_y) - (ab_y * bc_x)\n",
    "\n",
    "            # check for div by 0 issues\n",
    "            if xp_uz>0:\n",
    "                ua = u[ia]-((xp_ux*(xx-a[0])+xp_uy*(yy-a[1]))/max(xp_uz,0.001))\n",
    "            else:\n",
    "                ua = u[ia]-((xp_ux*(xx-a[0])+xp_uy*(yy-a[1]))/min(xp_uz,-0.001))\n",
    "            \n",
    "            if xp_vz>0:\n",
    "                va = v[ia]-((xp_vx*(xx-a[0])+xp_vy*(yy-a[1]))/max(xp_vz,0.001))\n",
    "            else:\n",
    "                va = v[ia]-((xp_vx*(xx-a[0])+xp_vy*(yy-a[1]))/min(xp_vz,-0.001))\n",
    "            \n",
    "            # if the point is not within the bounds of the nns \n",
    "            # trim to max of the neighbors\n",
    "            if ua>0:\n",
    "                ua = min(ua, max(u[ia],u[ib],u[ic]))\n",
    "            else:\n",
    "                ua = max(ua, min(u[ia],u[ib],u[ic]))\n",
    "\n",
    "            if va>0:\n",
    "                va = min(va, max(v[ia],v[ib],v[ic]))\n",
    "            else:\n",
    "                va = max(va, min(v[ia],v[ib],v[ic]))\n",
    "\n",
    "            # set the values in the results array\n",
    "            uv[i,0] = cp.float32(va)\n",
    "            uv[i,1] = cp.float32(ua)\n",
    "\n",
    "grid2 = cp.zeros((points.shape),dtype=cp.float32)\n",
    "\n",
    "cneighbors = cp.asarray(neighbors)\n",
    "cu = cp.array(u)\n",
    "cv = cp.array(v)\n",
    "\n",
    "#print(benchmark(elementwise_uv[128, 1024], (grid, cu, cv, cdistances, cneighbors, distances.shape[0]), n_repeat=10))  \n",
    "elementwise_uv[128, 1024](grid2, cu, cv, cneighbors, data, points, grid2.shape[0])  #  Numba style\n",
    "grid2.get()\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c0f3e7-6360-4f32-b1e4-4323d3405327",
   "metadata": {},
   "source": [
    "Amazingly, this kernel is able to speed up the computation by several orders of magnitude! For precise timings you can uncomment the benchmark call to the kernel, which averages several calls to the function to provide a more accurate performance figure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43425f99-f123-4870-b8bc-1d97896d9eef",
   "metadata": {
    "tags": []
   },
   "source": [
    "That is a lot faster!! If it actually works, then you'd expect that if we reverse the vector and apply it to the warped H&E image then it should return it to something like its original shape. Some pixels will have been lost because they were warped outside of the bounds of the image, but let's try it.\n",
    "\n",
    "First, though, we will plot the pixel offset map that this technique generated. As before blue and red channels are used to represent the horzontal and vertical vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bf9635-67ca-4dc9-ba3c-e456abf0b5c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coords = np.moveaxis(grid2.get().reshape(h1,w1,2),(0,1,2),(1,2,0))\n",
    "mx = np.max(coords.flatten())\n",
    "mn = np.min(coords.flatten())\n",
    "\n",
    "vec_img = np.zeros((h1,w1,3),dtype=np.uint8)\n",
    "vec_img[:,:,0] = (coords[0]-mn)*255/(mx-mn)\n",
    "vec_img[:,:,1] = 0\n",
    "vec_img[:,:,2] = (coords[1]-mn)*255/(mx-mn)\n",
    "\n",
    "plt.figure(figsize =(10, 10))\n",
    "\n",
    "# show plot\n",
    "plt.imshow(vec_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b01eb29-4641-400c-858e-139f393a402f",
   "metadata": {},
   "source": [
    "So the output shows that it is doing what is expected. Note that the edges of the nearest neighbor regions are less smooth that the skimage method we used before. This could be fixed in a few ways. However, the upside is that the anchor points remain in exactly the position they were defined at. We could smooth the output in several ways but this smoothing may slightly alter the anchor points we defined, so there are pros and cons.\n",
    "One method that could be explored would be to choose the four nearest points and then compute three sets of interpolations and take the average. Slightly more complex but it ought to smooth things out. Something to try when you have time? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc171d10-5151-47d2-a14e-9293833ff742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coords = cp.moveaxis(cp.negative(grid2).reshape(h1,w1,2),(0,1,2),(1,2,0))\n",
    "coords[1,:,:]+=cp.array(grid_x) \n",
    "coords[0,:,:]+=cp.array(grid_y)\n",
    "print(coords.shape)\n",
    "coords = cp.concatenate((coords,cp.zeros((1,h1,w1))),axis=0)\n",
    "\n",
    "coords = cp.stack((coords,coords,coords), axis=3)\n",
    "coords[2,:,:,0]=0.\n",
    "coords[2,:,:,1]=1.\n",
    "coords[2,:,:,2]=2.\n",
    "\n",
    "start = timer()\n",
    "unwarped_image = cu_map_coordinates(warped_image, coords)\n",
    "host_image = unwarped_image.get()\n",
    "end = timer()\n",
    "print(\"warp image - \",end-start)\n",
    "\n",
    "#plot the difference between the original and the unwarped\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(host_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f24bcd-4b1a-443d-83d1-f015d9db73ba",
   "metadata": {},
   "source": [
    "Now we can look at the difference between the original and the de-warped image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee2899-c79c-4694-9c2e-7969949a3595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(host_image-he_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96fefd3-7206-4518-a405-6590861cfd1e",
   "metadata": {},
   "source": [
    "So the quality of the actual unwarping isn't too bad, but the point is that it was performed extremely quickly. The algorithm can certainly be improved, but we have sped things up without having to leave the comfort of our Python armchair :)\n",
    "\n",
    "Okay, now we are going to load the DAPI channel image and resize it so that it matches the H & E. To do the resizing we will actually create a routine really just to show another easy source of acceleration, which is Numba's @jit feature. We can create kernels in a similar way to how we did previously with cupy, but perhaps the best acceleration to effort ratio can be acheived with one simple function decoration.\n",
    "First, run the code cell below as it is and try loading the DAPI image in the cell below and see how long in takes to load and process the whole image.\n",
    "Then, comment out the @jit line and do the same thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a8e62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numba import jit, prange\n",
    "\n",
    "# TODO - Comment out the line below to prevent Numba from compiling a parallel version of this function\n",
    "@jit(nopython=True, parallel=True)\n",
    "def resize_dapi(input_image, new_height, new_width):\n",
    "    \n",
    "    output_image = np.zeros((new_height, new_width), dtype=input_image.dtype)\n",
    "    old_height, old_width = input_image.shape\n",
    "    scale_factor_y = old_height/new_height\n",
    "    scale_factor_x = old_width/new_width\n",
    "\n",
    "    for new_y in prange(new_height):\n",
    "        old_y = new_y * scale_factor_y\n",
    "        y_fraction = old_y - math.floor(old_y)\n",
    "        for new_x in prange(new_width):\n",
    "            # use the scale factor to find the source pixels\n",
    "            old_x = new_x * scale_factor_x\n",
    "            x_fraction = old_x - math.floor(old_x)\n",
    "\n",
    "            # Sample four neighboring pixels:\n",
    "            left_upper = input_image[math.floor(old_y), math.floor(old_x)]\n",
    "            right_upper = input_image[math.floor(old_y), min(old_width - 1, math.ceil(old_x))]\n",
    "            left_lower = input_image[min(old_height - 1, math.ceil(old_y)), math.floor(old_x)]\n",
    "            right_lower = input_image[min(old_height - 1, math.ceil(old_y)), min(old_width - 1, math.ceil(old_x))]\n",
    "\n",
    "            # Interpolate horizontally:\n",
    "            blend_top = (right_upper * x_fraction) + (left_upper * (1.0 - x_fraction))\n",
    "            blend_bottom = (right_lower * x_fraction) + (left_lower * (1.0 - x_fraction))\n",
    "            # Interpolate vertically:\n",
    "            final_blend = (blend_top * y_fraction) + (blend_bottom * (1.0 - y_fraction))\n",
    "            output_image[new_y, new_x] = final_blend\n",
    "\n",
    "    return output_image\n",
    "\n",
    "def normalize(x, high=255):\n",
    "    y = x.flatten()\n",
    "    mx = np.max(y)\n",
    "    mn = np.min(y)\n",
    "    \n",
    "    return (x - mn)*high/(mx-mn)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f0a549-1ca7-4eb3-b72a-94457b784a08",
   "metadata": {},
   "source": [
    "So that has created the resizing and normalizing functions. Now we can load the DAPI channel image and apply these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c502c50e-bf57-477b-9b07-0678b1342ece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cupy\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "input_file2 = \"./data/morphology_mip.ome.3.invert.tif\"\n",
    "dp_img = np.array(tiff.imread(input_file2))\n",
    "\n",
    "# Dimensions of the whole Slide at full-resolution\n",
    "h2 = dp_img.shape[0]\n",
    "w2 = dp_img.shape[1]\n",
    "\n",
    "# resize to smaller of the 2 using linear interpolation\n",
    "dp_img = resize_dapi(dp_img[:,:], h1, w1)\n",
    "dp_img = normalize(dp_img,255).astype(np.uint8)\n",
    "\n",
    "# Show the image\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(dp_img)\n",
    "plt.title('morphology_mip.ome.3.invert.tif')\n",
    "print(\"Width = {}, Height = {}\".format(w2, h2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4083581-3ea5-4dbf-940e-d597c9800425",
   "metadata": {},
   "source": [
    "So, although the acquisition modality was different, you can still see that there are large scale features that correspond to the H & E.\n",
    "\n",
    "What you should have observed is that whilst the jit compiled version runs quickly, the non-jit version will take several minutes (Feel free to stop execution of that code cell, if you get bored). That's because, when you insert the @jit command, Numba compiles a version of the function and creates a parallel version of the loop code. \n",
    "\n",
    "Finally we are going to play with another tool that can also be used to speed up the processing of potentially expensive operations by distributing the code to any available cluster resources. In this case we are just going to use CPU threads, but the same process can be used for a GPU cluster too - which is very powerful. This is DASK and it has many uses and techniques but we will look at one simple example. What DASK does is to examine your code and figures out what dependencies exist between the various computations and builds an execution graph. This can be displyed using the following toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfc22ba-f3c0-4eb6-9280-72d937fc11d0",
   "metadata": {},
   "source": [
    "Imagine that we want to sum a series of integers. Naively, you'd have to iterate over each element one at a time adding each element to the running total. The run time would be a factor of the number elements. A better way would be to concurrently add every other element to its neighbour iteratively until there is only one element left. This would bring the runtime down to log(N) time. By providing a few basic commands you can let Dask figure out the execution graph for you. Let's look at a concrete example\n",
    "\n",
    "We can write the code to do the adding for us using a Dask Delayed function. This means that before the result is calculated a graph is constructed and Dask will map this graph onto the available compute (e.g. Processes, Threads or GPUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf76f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import delayed\n",
    "\n",
    "@dask.delayed\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "a = [i+1 for i in range(16)]\n",
    "b = []\n",
    "\n",
    "while len(a)>1:\n",
    "    for i in range(0,len(a),2):\n",
    "         b.append(add(a[i],a[i+1]))\n",
    "    a=b\n",
    "    b=[]\n",
    "    \n",
    "result = a[0]\n",
    "   \n",
    "result.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ec3ec4-8389-4d47-b7f2-647177863b91",
   "metadata": {},
   "source": [
    "At this point, no computation has been done - just the graph construction. By doing this up-front, a more efficient graph can be created. You can see that the graph shows how the additions at each phase can be done in parallel , but also how each subsequent addition depends only on its ancestors. To actually do the computation, we need to execute a compute() command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a295a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584cd71-370d-48df-a8e0-ef9bd3bcb303",
   "metadata": {},
   "source": [
    "Note that, in this toy example, the overhead of organizing the concurrency would far outweight any gains. This technique is really only suitable for larger problems. So, let's apply it to something a little more challenging.\n",
    "Let's compute the variance on the H & E and DAPI channels and see how they compare. The intuition being that, even if the modalities are different, we should see similar distributions of information variation across the images. This could provide some common ground with which to align them.\n",
    "We will use a sliding 8x8 window to compute the variance. We'll do it on the DAPI channel first.\n",
    "\n",
    "This time we create a Dask Cluster and Client, which allows us to use the handy dashboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2334bee7-e26f-4675-b230-ec39512960a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Setup a local cluster.\n",
    "cluster = LocalCluster(dashboard_address= 8787, processes=False)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7184927-3d87-497a-8abe-bcd591f62bd8",
   "metadata": {},
   "source": [
    "The dashboard URL above is fine for local installations, but when the server is hosted remotely it may not work. In this case, paste the URL generated in the cell below into the DASK DASHBOARD URL field in the Dask tool from the didebar tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed71698-df06-44f2-ae76-3b6b5b746893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "result = subprocess.run(['curl', '-s','ipecho.net/plain'], stdout=subprocess.PIPE)\n",
    "url = \"http://\" + str(result.stdout.decode('utf-8')) + \":8787\"\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e35ef6a-3e24-47d4-90d7-b4b2c5804f38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "sz=8\n",
    "\n",
    "def compute_variance(arr):\n",
    "    x1 = arr[0]\n",
    "    y1 = arr[1]\n",
    "    x2 = arr[0]+sz\n",
    "    y2 = arr[1]+sz\n",
    "    \n",
    "    nn_image[y1//sz,x1//sz]=dp_img[y1:y2,x1:x2].flatten().var()\n",
    "    \n",
    "nn_image = np.zeros((h1//sz,w1//sz),dtype=np.uint8)  \n",
    "\n",
    "\n",
    "start_loc_data = [(x, y) for x in range(0,w1-sz,sz) for y in range(0,h1-sz,sz)]\n",
    "        \n",
    "b = db.from_sequence(start_loc_data, npartitions=100)\n",
    "print(\"bagged\")\n",
    "b = b.map(compute_variance)\n",
    "print(\"mapped\")\n",
    "results_bag = b.compute()\n",
    "print(\"computed\")\n",
    "plt.imshow(nn_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b6313-b18b-4489-bfe3-a94293d3e7b7",
   "metadata": {},
   "source": [
    "Next we can do the same thing with the H&E image and see how they compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b350c307-677d-4455-be77-49dd1d181313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "\n",
    "def compute_variance(arr):\n",
    "    x1 = arr[0]\n",
    "    y1 = arr[1]\n",
    "    x2 = arr[0]+sz\n",
    "    y2 = arr[1]+sz\n",
    "    \n",
    "    n2_image[y1//sz,x1//sz]=he_img[y1:y2,x1:x2,2].flatten().var()\n",
    "    \n",
    "n2_image = np.zeros((h1//sz,w1//sz),dtype=np.uint8)  \n",
    "        \n",
    "b = db.from_sequence(start_loc_data, npartitions=200)\n",
    "print(\"bagged\")\n",
    "b = b.map(compute_variance)\n",
    "print(\"mapped\")\n",
    "results_bag = b.compute()\n",
    "print(\"computed\")\n",
    "plt.imshow(n2_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418fb798-68dd-448d-ae3d-65ad889fae52",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we can look at the similarity of the images to see whether this is a sufficiently common representation to use for registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c0dc85-c740-4c9c-ada6-134ab9dd389b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff = abs(nn_image-n2_image)\n",
    "print(max(diff.flatten()),min(diff.flatten()))\n",
    "      \n",
    "plt.imshow(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28630f55-f3c3-4a2e-af21-6273bad0697f",
   "metadata": {},
   "source": [
    "Although this operation is not especially well-suited to dask, it does show how dask can be used to speed up processes that might otherwise take a long time. \n",
    "\n",
    "The outputs of this might be used to idenfify points that would make good candidates for anchor points (i.e. those that correspond well between modalities). You could apply some some of thresholding, find the maxima and train a kernel that can find these points based on one of the modalities. However, that is a rich topic for another day...!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba5f15-2dad-4cee-bb72-d5ed60935e0c",
   "metadata": {},
   "source": [
    "# Final Exercise\n",
    "Given a convolution kernel, can you create an accelerated function that detects the highest correspondence between the R channel of the H&E and the DAPI channel?\n",
    "\n",
    "The motivation is that if we can detect regions of the image that are quite similar then we might be able to use these as anchor points. In reality, it might make sense to train a network to learn what type of kernel(s) provide the most similar activations across the two modalities. For this exercise you can just use a fixed kernel and pass it over the two images and see which locations produce the highest joint activation. This is computed simply by multiplying the summed activations together from each image at each location. Feel free to experiment with different convolution filters, but the main objective is to see how quickly you can do this for all the pixels of the image.\n",
    "Use whichever method(s) you prefer.\n",
    "\n",
    "A serial version of the code is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347e4610-3f1f-42d9-94f4-5bb0d5e8fb06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def joint_activation(h_image, d_image, k_filter):\n",
    "    \n",
    "    ksize = k_filter.shape[0]\n",
    "    new_height = h_image.shape[0]-k_filter.shape[0]+1\n",
    "    new_width = h_image.shape[1]-k_filter.shape[1]+1\n",
    "    output_image = np.zeros((new_height, new_width), dtype=np.float64)\n",
    "    \n",
    "    # reduced size to prevent excessive execution time!!!\n",
    "    # remove when you are confident of a speed up!!\n",
    "    new_height=1000\n",
    "    new_width=1000\n",
    "\n",
    "    for y in range(new_height):\n",
    "        for x in range(new_width):\n",
    "            sum1:np.float64=0.\n",
    "            for i in range(ksize):\n",
    "                for j in range(ksize):\n",
    "                    a = h_image[y+i,x+j] * kernel[i,j]\n",
    "                    b = d_image[y+i,x+j] * kernel[i,j]\n",
    "                    sum1=sum1 + a + b\n",
    "\n",
    "            output_image[y,x] = np.float64(sum1/255)\n",
    "            \n",
    "    return output_image\n",
    "\n",
    "kernel = np.array([[0.4,0.5,0.4],[0.5,1.,0.5],[0.4,0.5,0.4]])\n",
    "activation_image = joint_activation(he_img[:,:,0],dp_img[:,:],kernel)\n",
    "\n",
    "plt.imshow(activation_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9d41f5-fdf7-4dbf-8430-a08743ed5df9",
   "metadata": {},
   "source": [
    "Thank you for completing this lab today. We hope you enjoyed the content, learned something and will come back for more sessions in the future!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
